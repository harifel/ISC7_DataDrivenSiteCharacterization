{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb73a0-65aa-4212-bdb2-e36662cd5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from functions import plotting_raw_data, remove_outliers, error_plot, plot_cpt_data, plot_cpt_data_ML_prediction, plot_cpt_data_NW_site\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41e89e-74ac-41a5-94e1-1a64f2c713e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Define the text size of each plot globally ###########\n",
    "SMALL_SIZE = 10\n",
    "BIGGER_SIZE = 10\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "######################## Define the text size of each plot globally ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c9ea6-f875-43af-bc92-f84498f42867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Import CPT Dataset\n",
    "# =============================================================================\n",
    "\n",
    "# File path\n",
    "file_path = r\"..\\data\\CPT_PremstallerGeotechnik_revised.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_raw = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Select only SCPTu data\n",
    "df_SCPTu = df_raw[df_raw['test_type'] == 'SCPTu']\n",
    "# Select only SCPT data\n",
    "df_SCPT = df_raw[df_raw['test_type'] == 'SCPT']\n",
    "# Select both SCPTu and SPT data\n",
    "df_SCPTu_SCPT = df_raw[(df_raw['test_type'] == 'SCPTu') | (df_raw['test_type'] == 'SCPT')]\n",
    "df_SCPTu_SCPT_mean = df_raw[(df_raw['test_type'] == 'SCPTu') | (df_raw['test_type'] == 'SCPT')]\n",
    "\n",
    "selected_columns_x_average = ['Depth (m)','qc (MPa)', 'fs (kPa)', 'Rf (%)', 'u0 (kPa)', \"Ïƒ',v (kPa)\"]\n",
    "\n",
    "for column in selected_columns_x_average:\n",
    "    df_SCPTu_SCPT = df_SCPTu_SCPT.copy()\n",
    "    df_SCPTu_SCPT[column] = df_SCPTu_SCPT.loc[:, column].rolling(window=50).mean()\n",
    "\n",
    "    df_SCPTu_SCPT_mean = df_SCPTu_SCPT_mean.copy()\n",
    "    df_SCPTu_SCPT_mean[column+\"_mean\"] = df_SCPTu_SCPT_mean.loc[:, column].rolling(window=50).mean()\n",
    "\n",
    "df_SCPTu_SCPT = df_SCPTu_SCPT.dropna(subset=['Vs (m/s)'])\n",
    "df_SCPTu_SCPT_mean = df_SCPTu_SCPT_mean.dropna(subset=['Vs (m/s)'])\n",
    "\n",
    "# count number of tests in both subsets\n",
    "SCPTu_number = df_SCPTu['ID'].nunique()\n",
    "SCPT_number = df_SCPT['ID'].nunique()\n",
    "combined_number = df_SCPTu_SCPT['ID'].nunique()\n",
    "\n",
    "\n",
    "print('Preprocessing:\\n')\n",
    "print('Number of tests in SCPTu =', SCPTu_number)\n",
    "print('Number of tests in SCPT =', SCPT_number)\n",
    "print('Number of tests in SCPTu and SCPT =', combined_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaec4b0-fbdf-4d36-a663-0fee5eee9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Plotting the data and Selecting features\n",
    "# =============================================================================\n",
    "# Plotting CPT data \n",
    "cm = 1/2.54  # centimeters in inches\n",
    "\n",
    "# Select columns\n",
    "selected_columns_x = ['Depth (m)','qc (MPa)', 'fs (kPa)','Rf (%)','Vs (m/s)'] #for Machine learning features\n",
    "plot_columns_x_label = ['Depth (m)','$q_c$ (MPa)', '$f_s$ (kPa)','$R_f$ (%)', '$v_s$ (m/s)'] #for plotting purpose\n",
    "\n",
    "#Plot CPT: raw data and mean data\n",
    "unique_ids = df_SCPTu_SCPT_mean.loc[:,'ID'].unique()\n",
    "id_value = np.random.choice(unique_ids)\n",
    "plot_cpt_data((17*cm, 10*cm), selected_columns_x, df_raw,\n",
    "              df_SCPTu_SCPT_mean, id_value=id_value,\n",
    "              plot_columns_x_label=plot_columns_x_label)\n",
    "\n",
    "plt.savefig(f\"..\\\\graphics\\A_CPT_RAW_filterd_id_{id_value}.png\", dpi=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded3c44-d721-4c62-90a9-5c146622549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training data \n",
    "\n",
    "X = df_SCPTu_SCPT[selected_columns_x[:-1]]#.to_numpy()\n",
    "y = df_SCPTu_SCPT['Vs (m/s)']#.to_numpy()\n",
    "\n",
    "s = 1  # Adjust the marker size as needed\n",
    "color = 'k'  # Adjust the marker color as needed\n",
    "alpha = 0.5\n",
    "\n",
    "#Plot scatter points: raw data points as\n",
    "fig, axes = plt.subplots(4, 1, figsize=(8*cm, 20*cm), dpi=500, sharex=True)\n",
    "plotting_raw_data(X,y, alpha, s, color, 'Raw data', True, axes, plot_columns_x_label)\n",
    "\n",
    "########################### REMOVE outliers\n",
    "#df_SCPTu_SCPT = remove_outliers(df_SCPTu_SCPT, 'Vs (m/s)')\n",
    "#df_SCPTu_SCPT = df_SCPTu_SCPT[(df_SCPTu_SCPT['Vs (m/s)'] > 0)]\n",
    "\n",
    "#X = df_SCPTu_SCPT[selected_columns_x[:-1]]#.to_numpy()\n",
    "#y = df_SCPTu_SCPT['Vs (m/s)']#.to_numpy()\n",
    "########################### REMOVE outliers\n",
    "\n",
    "plotting_raw_data(X,y, alpha, s, 'r', 'Removed outliers', False, axes, plot_columns_x_label)\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"..\\\\graphics\\B_Raw_data.png\", dpi = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc045861-e60e-4860-b9f7-fdea70d11c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training of XGB model\n",
    "# =============================================================================\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.visualization import plot_param_importances, plot_optimization_history\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=2)\n",
    "\n",
    "def objective(trial):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    param = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 100),\n",
    "        'tree_method': 'hist',\n",
    "        \"verbosity\": 1,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.5, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"reg_alpha\":trial.suggest_float(\"reg_alpha\", 0.01, 1.0),\n",
    "        \"reg_lambda\":trial.suggest_float(\"reg_lambda\", 0.01, 1.0),\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**param, random_state=42)\n",
    "    score = cross_val_score(model, X=X_train, y=y_train, scoring=\"r2\", n_jobs=-1, cv=10, verbose=0)\n",
    "    \n",
    "    return score.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1090cbd-f301-4a47-8fbb-42b336f3173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'n_estimators': 20,\n",
    "    'tree_method': 'hist',\n",
    "    \"verbosity\": 1,\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.7,\n",
    "    'reg_alpha': 0.7,\n",
    "    'reg_lambda': 0.7,\n",
    "    'n_jobs': None,\n",
    "}\n",
    "\n",
    "# Update with the best hyperparameters\n",
    "best_params.update(study.best_params)\n",
    "\n",
    "# Create the final XGBRegressor with the best hyperparameters\n",
    "final_model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54acf711-90da-4e44-86ef-d605162c3656",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance of XGB ML model on the test data:\\n')\n",
    "# Check performance on test data\n",
    "y_pred = final_model.predict(X_test)\n",
    "# Calculate the R-squared score, Mean squared error\n",
    "score = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Test Data - R2: {round(score, 3)}, MSE: {round(mse, 3)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ce61a-638b-4c71-ac31-96e82933f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot((8*cm, 8*cm), y_test, y_pred, f'XGBRegressor; $R^2$: {round(score, 3)}, MSE: {round(mse, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0639a-f40b-4fa9-9717-4b3e3df0f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance of XGB ML model on train data:\\n')\n",
    "# Check performance on train data\n",
    "y_pred = final_model.predict(X_train)\n",
    "# Calculate the R-squared score, Mean squared error\n",
    "score = r2_score(y_train, y_pred)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(f'Training Data - R2: {round(score, 3)}, MSE: {round(mse, 3)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training of machine learning model\n",
    "# =============================================================================\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 100),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 50),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 0.01, 1),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.5, log=True),\n",
    "        'loss': \"squared_error\",\n",
    "    }\n",
    "\n",
    "    model = HistGradientBoostingRegressor(**params)\n",
    "    score = cross_val_score(model, X=X_train, y=y_train, scoring=\"r2\", n_jobs=-1, cv=10, verbose=0)\n",
    "    \n",
    "    return score.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15688776",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'max_depth': 1,\n",
    "    'max_leaf_nodes': 2,\n",
    "    'l2_regularization': 1,\n",
    "    'min_samples_leaf': 1,\n",
    "    'learning_rate': 1,\n",
    "    'loss': \"squared_error\",\n",
    "}\n",
    "\n",
    "# Update with the best hyperparameters\n",
    "best_params.update(study.best_params)\n",
    "\n",
    "# Create the final XGBRegressor with the best hyperparameters\n",
    "final_model = HistGradientBoostingRegressor(**best_params)\n",
    "\n",
    "# Train the final model on the entire training set\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57df35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance of HistGradientBoostingRegressor ML model:\\n')\n",
    "# Check performance on test data\n",
    "y_pred = final_model.predict(X_test)\n",
    "# Calculate the R-squared score, Mean squared error\n",
    "score = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Test Data - R2: {round(score, 3)}, MSE: {round(mse, 3)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot((8*cm, 8*cm), y_test, y_pred, f'HistGradientBoostingRegressor; R2: {round(score, 3)}, MSE: {round(mse, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance on train data\n",
    "y_pred = final_model.predict(X_train)\n",
    "# Calculate the R-squared score, Mean squared error\n",
    "score = r2_score(y_train, y_pred)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(f'Training Data - R2: {round(score, 3)}, MSE: {round(mse, 3)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cf4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "check_column = []\n",
    "check = 0\n",
    "\n",
    "for column in selected_columns_x[:-1]:\n",
    "    if df_SCPTu_SCPT[column].isnull().any():\n",
    "        check_column.append('nan')\n",
    "        check = 0\n",
    "    else:\n",
    "        check_column.append('non_nan')\n",
    "        check = 1\n",
    "\n",
    "    if check == 0:\n",
    "        # Handle the case when at least one column contains NaN values\n",
    "        pass\n",
    "    elif check == 1:\n",
    "        # Drop rows with NaN values in the selected columns\n",
    "        df_SCPTu_SCPT.dropna(subset=selected_columns_x[:-1], inplace=True)\n",
    "\n",
    "X = df_SCPTu_SCPT[selected_columns_x[:-1]]\n",
    "y = df_SCPTu_SCPT['Vs (m/s)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 1, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 50),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 50),\n",
    "        'max_samples': trial.suggest_int('max_samples', 1, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 1, 50),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
    "        'n_jobs': None,\n",
    "        'criterion': \"squared_error\",\n",
    "    }\n",
    "    model = RandomForestRegressor(**params)\n",
    "    score = cross_val_score(model, X=X_train, y=y_train, scoring=\"r2\", n_jobs=-1, cv=10, verbose=0)\n",
    "    \n",
    "    return score.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40278feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'n_estimators': 1,\n",
    "    'max_depth': 1,\n",
    "    'max_leaf_nodes': 2,\n",
    "    'max_samples': 1,\n",
    "    'min_samples_split': 1,\n",
    "    'min_samples_leaf': 1,\n",
    "    'n_jobs': None,\n",
    "    'criterion': \"squared_error\",\n",
    "}\n",
    "\n",
    "# Update with the best hyperparameters\n",
    "best_params.update(study.best_params)\n",
    "\n",
    "# Create the final XGBRegressor with the best hyperparameters\n",
    "final_model = RandomForestRegressor(**best_params,random_state=42)\n",
    "\n",
    "# Train the final model on the entire training set\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee253820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performance of RandomForestRegressor ML model:\\n')\n",
    "# Check performance on test data\n",
    "y_pred = final_model.predict(X_test)\n",
    "# Calculate the R-squared score, Mean squared error\n",
    "score = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Test Data - R2: {round(score, 3)}, MSE: {round(mse, 3)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_plot((8*cm, 8*cm), y_test, y_pred, f'RandomForestRegressor; R2: {round(score, 3)}, MSE: {round(mse, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b43579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance on train data\n",
    "y_pred = final_model.predict(X_train)\n",
    "# Calculate the R-squared score, Mean squared error\n",
    "score = r2_score(y_train, y_pred)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(f'Training Data - R2: {round(score, 3)}, MSE: {round(mse, 3)}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
